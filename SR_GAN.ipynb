{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmed-A-Salem/SuperResolution-GANs/blob/main/SR_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "9fiCYB8g24lm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvMJyu9N2rMf",
        "outputId": "e343941d-b32a-43b7-aad1-1e1f329e5c52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-9ea46a64f410>:36: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import PIL\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import add,Dense\n",
        "from tensorflow.keras.layers import UpSampling2D\n",
        "from tensorflow.keras.layers import LeakyReLU, PReLU\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Activation,Flatten\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "from IPython.display import display\n",
        "from skimage.transform import rescale, resize\n",
        "# import georasters as gr\n",
        "from matplotlib import image\n",
        "from matplotlib import colors\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import imageio\n",
        "# import rasterio\n",
        "import argparse\n",
        "# from rasterio.plot import reshape_as_image\n",
        "from skimage import img_as_ubyte\n",
        "import random\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "np.random.seed(10)\n",
        "tf.config.experimental_run_functions_eagerly(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "nVgnSYME286P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "u_g7B6c43cfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cropping"
      ],
      "metadata": {
        "id": "wCokHg2B3IHS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g2BalC12rMm"
      },
      "outputs": [],
      "source": [
        "# #create list of crops with coordinates Top-Left\n",
        "# def select_crops(initial_width,initial_height,target_size=384):\n",
        "#     crop_rect = []\n",
        "#     initial_width = initial_width - target_size\n",
        "#     initial_height = initial_height - target_size\n",
        "#     for x in range(0,initial_width,target_size):\n",
        "#         for y in range(0,initial_height,target_size):\n",
        "#             crop_rect.append({\"left\": x, \"top\": y})\n",
        "#     print(\"total number of crops= \", len(crop_rect))\n",
        "#     return crop_rect\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert Gtiff to png"
      ],
      "metadata": {
        "id": "PkU5mFum3MSU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raFgZV4q2rMn"
      },
      "outputs": [],
      "source": [
        "# def convert_s2(input_path, output_path, crop_rect, target_size, bands, rgb_mode=False,s2_file_name=None):\n",
        "#     with rasterio.open(input_path) as s2_data:\n",
        "#         s2 = s2_data.read(bands)\n",
        "#     cat_image = reshape_as_image(s2)\n",
        "\n",
        "#     if s2_file_name is None :\n",
        "#         file_name = input_path\n",
        "#     else:\n",
        "#         file_name = s2_file_name\n",
        "#     file_name = file_name.split(\"/\")[-1].split(\".\")[0]\n",
        "#     file_name = os.path.join(output_path, file_name)\n",
        "\n",
        "#     for index in range(len(crop_rect)):\n",
        "#         left = crop_rect[index][\"left\"]\n",
        "#         top = crop_rect[index][\"top\"]\n",
        "#         right = left + target_size\n",
        "#         bottom = top + target_size\n",
        "#         new_image = cat_image[top:bottom, left:right]\n",
        "#         new_image = np.moveaxis(new_image, -1, 0)\n",
        "#         new_file_name = file_name + \"_\" + str(index) + \".tif\"\n",
        "#         if rgb_mode:\n",
        "#             display_channels = np.array(bands) - 2\n",
        "#             display_channels = display_channels.tolist()\n",
        "#             s2 = new_image.astype(np.float32)\n",
        "#             min_value, max_value = np.min(s2), np.max(s2)\n",
        "#             s2 = np.clip(s2, min_value, max_value)\n",
        "#             s2 /= max_value\n",
        "#             s2 = s2.astype(np.float32)\n",
        "#             #s2 = s2[display_channels, :, :]\n",
        "#             rgb = np.rollaxis(s2, 0, 3)\n",
        "#             imageio.imsave(new_file_name.replace('tif', 'png'), img_as_ubyte(rgb))\n",
        "\n",
        "#         else:\n",
        "#             with rasterio.open(new_file_name, 'w',width=target_size, height=target_size,\n",
        "#                                count=len(bands),driver=\"Gtiff\", dtype=rasterio.uint16) as dst:\n",
        "#                 dst.write(new_image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wecwt0F2rMp"
      },
      "outputs": [],
      "source": [
        "# def cut_sen2_files(target_size=384):\n",
        "#     path = '/data/super_resolution/original/'       # path of original hr images\n",
        "#     output_path = '/data/super_resolution/output'   #save high resolution data after cropping here\n",
        "#     for p in os.listdir(path):\n",
        "#         f= gr.from_file(path+p)\n",
        "#         file_name = p\n",
        "#         ch,initial_height, initial_width = f.shape\n",
        "#         crop_rect = select_crops(initial_width, initial_height, target_size)\n",
        "#         bands = [5, 3, 2]\n",
        "#         convert_s2(path+file_name, output_path ,crop_rect, target_size, bands, rgb_mode=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calling Preprocessing Functions"
      ],
      "metadata": {
        "id": "RyWZ8alC3peA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cut_sen2_files()"
      ],
      "metadata": {
        "id": "xoFuPRpURrcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Denoise & best Contour"
      ],
      "metadata": {
        "id": "wcvCH8CpAp0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path_hr = '/content/outputhr/'\n",
        "# path_hr_processed = '/content/outputhr_processed/'\n",
        "# def denoise_contour():\n",
        "#   for p in os.listdir(path_hr):\n",
        "#     img = cv2.imread(path_hr+p)\n",
        "#     noiseless_image_colored = cv2.fastNlMeansDenoisingColored(img,None,20,20,7,21)\n",
        "#     gray = cv2.cvtColor(noiseless_image_colored,cv2.COLOR_BGR2GRAY)\n",
        "#     _,thresh = cv2.threshold(gray,1,255,cv2.THRESH_BINARY)\n",
        "#     contours,hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
        "#     cnt = contours[0]\n",
        "#     x,y,w,h = cv2.boundingRect(cnt)\n",
        "#     crop = noiseless_image_colored[y:y+h,x:x+w]\n",
        "#     img = Image.fromarray(crop)\n",
        "#     img.save(path_hr_processed+p, 'png')"
      ],
      "metadata": {
        "id": "cAj3iCG6--Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wCaDAzR2rMp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSlfeHR72rMq"
      },
      "outputs": [],
      "source": [
        "# lr_shape=(96, 96, 3)\n",
        "# #path of cropped hr images\n",
        "# path = '/data/super_resolution/output/'\n",
        "# # path that you need to save lr images in\n",
        "# outputpath = '/data/super_resolution/input/'\n",
        "# for f in os.listdir(path):\n",
        "#     image = Image.open(path+f) \n",
        "#     lr_resized=np.array(image.resize((lr_shape[0],lr_shape[1]),PIL.Image.BICUBIC))\n",
        "#     img = Image.fromarray(lr_resized)\n",
        "#     img.save(outputpath+f, 'png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CLGDFnj2rMr",
        "outputId": "30361de4-7f83-4c4d-c369-d3a15d64d721"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "def load_images_from_folder(folder): \n",
        "    images = [] \n",
        "    for filename in os.listdir(folder): \n",
        "        if (filename == '.ipynb_checkpoints'):\n",
        "          continue\n",
        "        img = Image.open(os.path.join(folder,filename)) \n",
        "        img_resized=np.array(img)\n",
        "        images.append(img_resized) \n",
        "    return images\n",
        "\n",
        "output= load_images_from_folder('/content/hr')\n",
        "input= load_images_from_folder('/content/lr')\n",
        "len(input), len(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcJbpNqw-OZT",
        "outputId": "bb015717-6f65-4176-d04d-cb1a1b982981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 384, 384, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TGk-g_52rMt"
      },
      "outputs": [],
      "source": [
        "input_ = np.array(input)\n",
        "output_ = np.array(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya58QMqE2rMu"
      },
      "outputs": [],
      "source": [
        "def normalize_img(input_data):\n",
        "    return (input_data.astype(np.float32) - 127.5)/127.5 \n",
        "\n",
        "def denormalize(input_data):\n",
        "    input_data = (input_data + 1) * 127.5\n",
        "    return input_data.astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AFs2OIq2rMv",
        "outputId": "5977cac4-b557-4b35-e936-75672b94f048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "input = normalize_img(input_)\n",
        "output = normalize_img(output_)\n",
        "print(len(input)), print(len(output))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "TUu5pvzL5nsC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPKkvwbK2rMz",
        "outputId": "504d8130-c224-40ee-b9a4-1890a0691e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 1\n"
          ]
        }
      ],
      "source": [
        "X_train_lr, X_test_lr, X_train_hr, X_test_hr = train_test_split(input, output, test_size=0.5, random_state=42)\n",
        "print(len(X_train_lr), len(X_train_hr))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Training Functions"
      ],
      "metadata": {
        "id": "V0wlz-Iu5I8o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQ52Sxkb2rM0"
      },
      "outputs": [],
      "source": [
        "###Upsample function(changing from H * W * C to H * W * 4C then to 2H * 2W * C using pixelshuffling)\n",
        "def upsample(model,filter_size,no_of_channels,strides):\n",
        "    #scaling factor=2\n",
        "    scale=2\n",
        "    no_of_filters=no_of_channels *(scale ** 2)\n",
        "    model=Conv2D(filters=no_of_filters,kernel_size=filter_size,strides=strides,padding='same')(model)\n",
        "    model=UpSampling2D(size=scale)(model)\n",
        "    model=PReLU()(model)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujqtT39m2rM0"
      },
      "outputs": [],
      "source": [
        "def residual_block(model, kernel_size, no_of_filters, strides):  \n",
        "    gen = model\n",
        "    model = Conv2D(filters = no_of_filters, kernel_size = kernel_size, strides = strides, padding = \"same\")(model)\n",
        "    model = BatchNormalization(momentum = 0.5)(model)\n",
        "    \n",
        "    # Using Parametric ReLU\n",
        "    model = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(model)\n",
        "    model = Conv2D(filters = no_of_filters, kernel_size = kernel_size, strides = strides, padding = \"same\")(model)\n",
        "    model = BatchNormalization(momentum = 0.5)(model)\n",
        "    \n",
        "    model = add([gen, model])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator Network"
      ],
      "metadata": {
        "id": "690p8q_B5SV3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aEGzbl32rM1"
      },
      "outputs": [],
      "source": [
        "# Using Functional API of Keras\n",
        "\n",
        "def generator_network(gen_input):\n",
        "\n",
        "    gen_input = Input(shape = gen_input)\n",
        "     \n",
        "    model = Conv2D(filters = 64, kernel_size = 9, strides = 1, padding = \"same\")(gen_input)\n",
        "    model = PReLU(shared_axes=[1,2])(model)               # each filter only has one set of parameters\n",
        "    model_part1 = model\n",
        "        \n",
        "  # Using 16 Residual Blocks\n",
        "    for i in range(16):\n",
        "        model = residual_block(model, 3, 64, 1)\n",
        "        # 16 residual blocks with skip connections \n",
        "\n",
        "    model = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = \"same\")(model)\n",
        "    model = BatchNormalization(momentum = 0.5)(model)\n",
        "    model = add([model_part1, model])                      \n",
        "  #  Element wise of model_part1 and model after 16 residual blocks\n",
        "     \n",
        "\n",
        "    for i in range(2):\n",
        "        model = upsample(model, 3, 64, 1)  #no of channels=64  \n",
        "     \n",
        "    model = Conv2D(filters = 3, kernel_size = 9, strides = 1, padding = \"same\")(model)\n",
        "  \n",
        "    model = Activation('tanh')(model)                  # tanh activation in last layer\n",
        "    \n",
        "    gen_model = Model(inputs = gen_input, outputs = model)     # specifying the input and output to the model\n",
        "  \n",
        "    return gen_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discriminator Network"
      ],
      "metadata": {
        "id": "8D8qdfCs5W8b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFP1MMnf2rM1"
      },
      "outputs": [],
      "source": [
        "def conv_disc_block(model,filters,filter_size,strides):\n",
        "    model=Conv2D(filters=filters,kernel_size=filter_size,strides=strides,padding='same')(model)\n",
        "    model = BatchNormalization(momentum = 0.5)(model)\n",
        "    model=LeakyReLU(alpha=0.1)(model)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrjJq_Td2rM1"
      },
      "outputs": [],
      "source": [
        "def discriminator_network(image_shape):\n",
        "    disc_input=Input(shape = image_shape)\n",
        "  #convolution layer(k3n64s1)\n",
        "    model=Conv2D(filters = 64,kernel_size = 3,strides=1,padding='same' )(disc_input)\n",
        "  #Activation-leaky relu\n",
        "    model=LeakyReLU(alpha=0.1)(model)\n",
        "  \n",
        "  #discriminator block (k3n64s2)\n",
        "    model=conv_disc_block(model,64,3,2)\n",
        "  #discriminator block (k3n128s1)\n",
        "    model=conv_disc_block(model,128,3,1)\n",
        "  #discriminator block (k3n128s2)\n",
        "    model=conv_disc_block(model,128,3,2)\n",
        "  #discriminator block (k3n256s1)\n",
        "    model=conv_disc_block(model,256,3,1)\n",
        "  #discriminator block (k3n256s2)\n",
        "    model=conv_disc_block(model,256,3,2)\n",
        "  #discriminator block (k3n512s1)\n",
        "    model=conv_disc_block(model,512,3,1)\n",
        "  #discriminator block (k3n512s2)\n",
        "    model=conv_disc_block(model,512,3,2)\n",
        "\n",
        "  #for dense layer input should be column vector/flatten\n",
        "    model=Flatten()(model)\n",
        "  #dense layer with 1024 nodes\n",
        "    model=Dense(1024)(model)\n",
        "  #Activation-leaky relu\n",
        "    model=LeakyReLU(alpha=0.1)(model)\n",
        "\n",
        "  #dense layer with 1 nodes\n",
        "    model=Dense(1)(model)\n",
        "  #Activation-sigmoid\n",
        "    model=Activation('sigmoid')(model)\n",
        "    disc_model=Model(inputs=disc_input,outputs=model)\n",
        "    return disc_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GAN Network"
      ],
      "metadata": {
        "id": "g8mug3SH6dor"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R830ZCln2rM2"
      },
      "outputs": [],
      "source": [
        "def gan_network(generator_model,discriminator_model,shape):\n",
        "    discriminator_model.trainable = False\n",
        "    gan_input=Input(shape=shape)\n",
        "    print(\"input\")\n",
        "    SR=generator_model(gan_input)\n",
        "    print(\"SR\")\n",
        "    gan_output=discriminator_model(SR)\n",
        "    print(\"gan_output\")\n",
        "    model=Model(inputs=gan_input,outputs=[SR,gan_output])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Functions"
      ],
      "metadata": {
        "id": "5ESZKLHu52ZR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGqsz_Eu2rM2"
      },
      "outputs": [],
      "source": [
        "# To use mean, square we need to use keras.backend\n",
        "# For content loss, compare the results which the vgg19 provides which feeding the y_true and y_pred\n",
        "def content_loss(image_shape):\n",
        "    def loss( y_true, y_pred):\n",
        "        vgg19 = VGG19(include_top=False, weights='imagenet', input_shape=image_shape)\n",
        "        vgg19.trainable = False\n",
        "        for layer in vgg19.layers:\n",
        "            layer.trainable = False\n",
        "        model = Model(inputs=vgg19.input, outputs=vgg19.get_layer('block5_conv4').output)\n",
        "        model.trainable = False\n",
        "        return K.mean(K.square(model(y_true) - model(y_pred)))\n",
        "    return loss "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot"
      ],
      "metadata": {
        "id": "hMWwsBU_660K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PtgiWBo2rM3"
      },
      "outputs": [],
      "source": [
        "def plot_generated_images(epoch,generator, examples=3 , dim=(1, 3), figsize=(15, 5)):  \n",
        "    \n",
        "    rand_nums = np.random.randint(0, X_test_hr.shape[0], size=examples)\n",
        "    image_batch_hr = denormalize(X_test_hr[rand_nums])\n",
        "    image_batch_lr = X_test_lr[rand_nums]\n",
        "    gen_img = generator.predict(image_batch_lr)\n",
        "    generated_image = denormalize(gen_img)\n",
        "    image_batch_lr = denormalize(image_batch_lr)\n",
        "    \n",
        "    \n",
        "    plt.figure(figsize=figsize)\n",
        "    \n",
        "    plt.subplot(dim[0], dim[1], 1)\n",
        "    plt.imshow(image_batch_lr[1], interpolation='nearest')\n",
        "    plt.axis('off')\n",
        "        \n",
        "    plt.subplot(dim[0], dim[1], 2)\n",
        "    plt.imshow(generated_image[1], interpolation='nearest')\n",
        "    plt.axis('off')\n",
        "    \n",
        "    plt.subplot(dim[0], dim[1], 3)\n",
        "    plt.imshow(image_batch_hr[1], interpolation='nearest')\n",
        "    plt.axis('off')\n",
        "    \n",
        "    plt.tight_layout() "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Train"
      ],
      "metadata": {
        "id": "Jfvwnv8Q7Qyv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI-4ZNk52rM4"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "hr_shape=(384,384,3)\n",
        "lr_shape=(hr_shape[0]/4,hr_shape[1]/4,hr_shape[2])\n",
        "\n",
        "def train_model(batch_size,epochs):\n",
        "    no_of_batches=X_train_hr.shape[0]//batch_size\n",
        "    adam = Adam(lr=0.0001 ,beta_1=0.9 ,beta_2=0.999, epsilon=1e-08 )\n",
        "    discriminator_model = discriminator_network(hr_shape)\n",
        "    generator_model = generator_network(lr_shape)\n",
        "    generator_model.compile(loss=content_loss(hr_shape), optimizer=adam)\n",
        "    discriminator_model.compile(loss='binary_crossentropy',optimizer=adam)\n",
        "\n",
        "    gan_model=gan_network(generator_model,discriminator_model,lr_shape)\n",
        "    discriminator_model.trainable = False\n",
        "    gan_model.compile(loss=[content_loss(hr_shape),'binary_crossentropy'],loss_weights=[1.0,1e-3],optimizer=adam)\n",
        "\n",
        "\n",
        "    for i in range(0,epochs):\n",
        "        print(\"\\nEpoch    : \"+ str(i))\n",
        "\n",
        "        for j in range(no_of_batches):\n",
        "\n",
        "            print(\"Batch    : \"+str(j))\n",
        "\n",
        "            rand_nums = np.random.randint(0, X_train_hr.shape[0], size=batch_size)\n",
        "\n",
        "            image_batch_hr = X_train_hr[rand_nums]\n",
        "            image_batch_lr = X_train_lr[rand_nums]\n",
        "\n",
        "            batch_gen_sr = generator_model.predict(image_batch_lr)\n",
        "            real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2      ## Here we use concept of label smoothing\n",
        "            fake_data_Y = np.random.random_sample(batch_size)*0.2\n",
        "\n",
        "            discriminator_model.trainable = True\n",
        "            d_loss_real = discriminator_model.train_on_batch(image_batch_hr, real_data_Y)\n",
        "            d_loss_fake = discriminator_model.train_on_batch(batch_gen_sr, fake_data_Y)\n",
        "            d_loss = 0.5*np.add(d_loss_fake, d_loss_real)\n",
        "\n",
        "            discriminator_model.trainable = False     \n",
        "            gan_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2\n",
        "            loss_gan = gan_model.train_on_batch(image_batch_lr, [image_batch_hr,gan_data_Y])\n",
        "\n",
        "\n",
        "        print(\"discriminator_loss : %f\" % d_loss)\n",
        "        print(\"gan_loss :\", loss_gan)\n",
        "        loss_gan = str(loss_gan)\n",
        "\n",
        "        # loss_file = open( '/kaggle/working/losses.txt' , 'a')\n",
        "        # loss_file.write('epoch%d : gan_loss = %s ; discriminator_loss = %f\\n' %(i, loss_gan, d_loss) )\n",
        "        # loss_file.close()\n",
        "\n",
        "        plot_generated_images(i, generator_model)\n",
        "\n",
        "        generator_model.save('/kaggle/working/Super-Resolve/gen_model.h5')\n",
        "        # discriminator_model.save('/kaggle/working/Super-Resolve/dis_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run and Results"
      ],
      "metadata": {
        "id": "JtTepn6-7zXt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVasYp8X2rM6"
      },
      "outputs": [],
      "source": [
        "lr_shape = tuple(map(int, lr_shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdJFckc92rM7"
      },
      "outputs": [],
      "source": [
        "# Not Enough RAM to train on kaggle, but I have successfully trained it on colab\n",
        "# To train with batch size 4 and epochs 50\n",
        "\n",
        "train_model(1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RTgjLx1MGcvj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}